# Do Your Homework Rubric #

## Paper Title ##: DEEPCASE: Semi-Supervised Contextual Analysis of Security Events
## Authors ##: Thijs van Ede∗†, Hojjat Aghakhani†, Noah Spahn†, Riccardo Bortolameotti‡, Marco Cova§, Andrea Continella∗, Maarten van Steen∗, Andreas Peter∗, Christopher Kruegel†, Giovanni Vigna†

### <strong>DATASET</strong> ###

* What is the data they use to train?( Brief description)
    *   2 Datasets of sequences of Security events:
    *   Real-world data -> VMWare/Lastline Dataset.
    *   Synthetic data -> HDFS Dataset. 

* Can you immediately Download the Dataset?
    * Lastline: No, requires NDA.
    * HDFS: Yes, GitHub ->  https://github.com/wuyifan18/DeepLog/tree/master/data.

    * If yes, is the dataset in a CSV or folder? 
        * Yes, the HDFS dataset is in the form of a text file with numeric training data separated by tab so it can be converted into CSV.
    * If no, do they have any method for obtaining the data? (e.g., “request access to data from …” or “make an account at …”)
        * No, no link to the dataset is given nor are any authors of the HDFS dataset mentioned that may be contacted.
* Is the Data Described?
    * Do they provide a paragraph description of their data?
        * Yes, Both datasets are explained in detail.
    * If data is featurized, Do they provide all of the features or columns of the data? 
        * Yes, but it's different from the traditional way. The authors have made CSV files with features and JSON files with mappings of features to the data files. https://github.com/Thijsvanede/DeepCASE/tree/main/mapping/mappings
    * If they created the dataset, Do they list where they collected it?
        * They didn’t create a dataset but mentioned where they collected it from in the case of the HDFS dataset.
    * Do they list the size of the dataset? (Total Samples)
        * Yes,
        * Lastline -> 10.5M security events for 291 unique types of security events.
        * HDFS -> 11.2M system log entries generated by over 200 Amazon EC2 nodes.

    * Do they describe the range of each feature?
        * Discrete range: The range of events is based on a triage system where security events are either classified into 6 categories: INFO, LOW, MEDIUM, HIGH, ATTACK, and SUSPICIOUS.
* How many datasets are used? (If more than one, provide citation if not created)
    * 2 datasets.
    * Lastline: No, requires NDA.
    * HDFS: Yes, GitHub ->  https://github.com/wuyifan18/DeepLog/tree/master/data.

    * If a dataset is included from different work, Can you download that dataset?
        * HDFS: Yes
* Do they describe any preprocessing steps? (PCA, scaling, normalziation)
    * The numeric events are mapped against the text to categorize them.
    * The events are passed in the form of a sliding window to context builder with an additional filter that the window has events within the range of 1 day, here n = 10 and left padded in case the events are less.
    * One hot encoding is applied to embeddings.

* Do they split their data into training/validation/test?
    * Yes
    * If yes, do they use a separate distinct set for a test set?
        * No, Made from the same data set split by months.
    * Do they use any cross-validation?
        * No
    * Is the downloaded data separated into train/val/test?
        * Yes, Train, Test_normal, Test_abnormal sets but no validation set separately.
    * If not, do they provide a random seed for their splitting?
        * N/A

###<strong>CODE</strong>###

* Is the Code linked in the paper?
    * Yes -> https://github.com/Thijsvanede/DeepCASE/tree/main/deepcase
* Is the Code linked to author website? (Github)
    * Yes -> https://github.com/Thijsvanede/DeepCASE
* Do they link a compiled app?
    * Yes -> https://deepcase.readthedocs.io/en/latest/reference/deepcase/deepcase.html
* Did they do hyperparameter tuning?
    * Yes, using backpropagation the weights and bias are self-tuned in NN.
    * If yes: Did they perform a gridsearch or random search?
        * Yes, 10-fold grid search on the first 1% of data of the VMWARE dataset sorted by time.

    * Do they list the parameters with which a search was performed? (Range of features)
        * Yes, they are mentioned in a table showing parameters and their corresponding values which show the range of sequence length, time, hidden dimension, sigma, tau, epsilon, and minimum sequences values and ranges.
* Do they describe the method of the model in the paper?
    * Yes
* Is there a visualization of the model? 
    * Yes
* Do they describe the types of layers they use? (Convolutional, LSTM, Dense)
    * Yes, the Embedding layer, Linear Layer, Softmax, and Matrix Multiplication are used where GRU is used in the recurrent layer.
* What is their activation function? (ReLU, Sigmoid, Tanh, etc)
    * RELU and SOFTMAX.
* What is their loss function?
    * Mean Squared Error with label smoothing is the loss function.
    * For backpropagation in NN, Kullback-Leibler divergence is a loss function.

* Do they provide how long it was trained for? (Epochs not time)
    * Context Builder takes 100 epochs.
* How long did it take to train their model? (hours, days, …)
    * 1 epoch takes 1.3s with HDFS and 13.8min for Lastline. For 1 year of data, 1.2 Million events, it less than 5 min to train.
* Do they provide justification for how long it was trained? (Earlystopping, chose arbitrary # of epochs)
    * Yes, they claim that this was possible due to highly parallelized GPUs, and instead of using DBSCAN for clustering which is  O(n^2), they used KD-tree with O(n*logn) complexity.
* Do they describe the computing resources they used?
    * Yes, 
    * Experiments ran on Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz machine running on Ubuntu 18.04 LTS. 
    * Neural Network Training and Prediction ran on an NVIDIA TITAN RTX 24 GB TU102 graphics card.

* What are the batch sizes they use?
    * default Batch size to use for training =  128 
* Do they say how their batches are generated (randomly sampled, special technique)?
    * No
* Do they use batch normalization?
    * Yes, In the attention decoder, the context vector is normalized from 0 to 1 using the Softmax function to form the attention vector.
* If there is code:
    * Does the repository have any code in it?
        * Yes
    * Can you download it?
        * Yes
    * Is there a README.md?
        * Yes
    * Is there an environment saved?
        * Will need to check
    * Do they list all libraries and versions of the library needed?
        * Yes
    * Is there a saved model?
        * Yes
    * Is their code commented?
        * Yes
    * Do you need to re-train/further train the model?
        * Will need to check
    * Can you run a command to start training?
        * Yes
    * Can you run a command to predict on the old data?
        * Yes
    * Can you predict one new sample?
        * Yes
    * Do they include all relevant code? (Model training, preprocessing, predicting)
        * Yes
    * If alternate data formats (jpg vs png, flac vs wav), does the code handle new data format?
        * No, only .csv or .txt files.
    * Is their code optimized for GPU?
        * Yes
    * When running their code on the dataset, do you get the same results reported in the paper?
        * Havnt run the code yet













